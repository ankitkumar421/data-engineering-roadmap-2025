{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3850167f-e2c5-45e8-8ae1-f51b5f49b343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Created\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Week3_Day5_PySpark_SQL_Time_Analytics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4011b967-0b2a-47b6-a9e6-93deaca2232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+------+\n",
      "|order_id|customer_id|order_date         |amount|\n",
      "+--------+-----------+-------------------+------+\n",
      "|1       |101        |2024-01-01 10:15:00|500   |\n",
      "|2       |102        |2024-01-01 11:00:00|700   |\n",
      "|3       |101        |2024-01-02 09:45:00|300   |\n",
      "|4       |103        |2024-01-03 14:20:00|900   |\n",
      "|5       |102        |2024-01-03 17:10:00|400   |\n",
      "+--------+-----------+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "\n",
    "data = [\n",
    "    Row(order_id=1, customer_id=101, order_date=\"2024-01-01 10:15:00\", amount=500),\n",
    "    Row(order_id=2, customer_id=102, order_date=\"2024-01-01 11:00:00\", amount=700),\n",
    "    Row(order_id=3, customer_id=101, order_date=\"2024-01-02 09:45:00\", amount=300),\n",
    "    Row(order_id=4, customer_id=103, order_date=\"2024-01-03 14:20:00\", amount=900),\n",
    "    Row(order_id=5, customer_id=102, order_date=\"2024-01-03 17:10:00\", amount=400)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data)\n",
    "df = df.withColumn(\"order_date\", to_timestamp(col(\"order_date\")))\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2a5858-8b87-4932-89fc-7bb104df6beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Temporary SQL View Created\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"orders\")\n",
    "print(\"✅ Temporary SQL View Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b4437e7-c6b5-492a-a7aa-8ce020b721cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|customer_id|total_amount|\n",
      "+-----------+------------+\n",
      "|        101|         800|\n",
      "|        102|        1100|\n",
      "|        103|         900|\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT customer_id, SUM(amount) AS total_amount\n",
    "FROM orders\n",
    "GROUP BY customer_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a47a13ce-4e87-4b73-82ae-1b9c28b26ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "| order_day|total_orders|daily_sales|\n",
      "+----------+------------+-----------+\n",
      "|2024-01-01|           2|       1200|\n",
      "|2024-01-02|           1|        300|\n",
      "|2024-01-03|           2|       1300|\n",
      "+----------+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DATE(order_date) AS order_day,\n",
    "       COUNT(order_id) AS total_orders,\n",
    "       SUM(amount) AS daily_sales\n",
    "FROM orders\n",
    "GROUP BY DATE(order_date)\n",
    "ORDER BY order_day\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da3377e6-c13b-41a7-97a7-d97b98aef33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+-------------------+------+-----------+-----+----+--------------+\n",
      "|order_id|customer_id|         order_date|amount|day_of_week|month|year|formatted_date|\n",
      "+--------+-----------+-------------------+------+-----------+-----+----+--------------+\n",
      "|       1|        101|2024-01-01 10:15:00|   500|          2|    1|2024|    2024-01-01|\n",
      "|       2|        102|2024-01-01 11:00:00|   700|          2|    1|2024|    2024-01-01|\n",
      "|       3|        101|2024-01-02 09:45:00|   300|          3|    1|2024|    2024-01-02|\n",
      "|       4|        103|2024-01-03 14:20:00|   900|          4|    1|2024|    2024-01-03|\n",
      "|       5|        102|2024-01-03 17:10:00|   400|          4|    1|2024|    2024-01-03|\n",
      "+--------+-----------+-------------------+------+-----------+-----+----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dayofweek, month, year, date_format\n",
    "\n",
    "df_dates = df.withColumn(\"day_of_week\", dayofweek(\"order_date\")) \\\n",
    "             .withColumn(\"month\", month(\"order_date\")) \\\n",
    "             .withColumn(\"year\", year(\"order_date\")) \\\n",
    "             .withColumn(\"formatted_date\", date_format(\"order_date\", \"yyyy-MM-dd\"))\n",
    "df_dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5470c291-6c5b-43b1-a5d2-18698b464808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-----------+\n",
      "|start              |end                |sum(amount)|\n",
      "+-------------------+-------------------+-----------+\n",
      "|2024-01-01 00:00:00|2024-01-02 00:00:00|1200       |\n",
      "|2024-01-02 00:00:00|2024-01-03 00:00:00|300        |\n",
      "|2024-01-03 00:00:00|2024-01-04 00:00:00|1300       |\n",
      "+-------------------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "df_window = df.groupBy(\n",
    "    window(\"order_date\", \"1 day\")\n",
    ").sum(\"amount\")\n",
    "\n",
    "df_window.select(\"window.start\", \"window.end\", \"sum(amount)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f0d0a-8da5-46b6-b171-c8fad9ea91cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
