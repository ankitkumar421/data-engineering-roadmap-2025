{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aef7abd-47b0-436d-bc27-126fac1fc5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Spark Session Ready\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Week3_Day3_PySpark_Storage_Optimization\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"✅ Spark Session Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "479283cd-e3eb-4d7b-ae97-57b0a57b25d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------------------+\n",
      "| id|category|             value|\n",
      "+---+--------+------------------+\n",
      "|  0|       A| 77.59360960017673|\n",
      "|  1|       B| 92.54651086956007|\n",
      "|  2|       C| 94.80803080062599|\n",
      "|  3|       D|32.060780506571675|\n",
      "|  4|       E| 38.49879434646528|\n",
      "+---+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Total rows: 1000000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df = spark.range(0, 1_000_000) \\\n",
    "    .withColumn(\"category\", expr(\"CASE WHEN id % 5 = 0 THEN 'A' WHEN id % 5 = 1 THEN 'B' WHEN id % 5 = 2 THEN 'C' WHEN id % 5 = 3 THEN 'D' ELSE 'E' END\")) \\\n",
    "    .withColumn(\"value\", expr(\"rand() * 100\"))\n",
    "\n",
    "df.show(5)\n",
    "print(f\"Total rows: {df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6af9de0d-357c-4308-8f9c-6934383a1670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cached in memory\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, category: string, value: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()\n",
    "df.count()  # triggers caching\n",
    "print(\"✅ Cached in memory\")\n",
    "\n",
    "# Try persistence (both memory and disk)\n",
    "from pyspark import StorageLevel\n",
    "df.persist(StorageLevel.MEMORY_AND_DISK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "379af852-94eb-4b1f-895a-77ade773bff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions before: 8\n",
      "Partitions after: 10\n"
     ]
    }
   ],
   "source": [
    "print(\"Partitions before:\", df.rdd.getNumPartitions())\n",
    "\n",
    "df_repart = df.repartition(10, \"category\")\n",
    "print(\"Partitions after:\", df_repart.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fac919e-4dd6-4ca1-a7db-9fc0740d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet\n",
    "df_repart.write.mode(\"overwrite\").parquet(\"output/day3_parquet\")\n",
    "\n",
    "# ORC\n",
    "df_repart.write.mode(\"overwrite\").orc(\"output/day3_orc\")\n",
    "\n",
    "# CSV\n",
    "df_repart.write.mode(\"overwrite\").csv(\"output/day3_csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ee61b71-1491-4d50-82df-338dc8e8aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARQUET read time: 1.3 sec\n",
      "ORC read time: 1.08 sec\n",
      "CSV read time: 2.1 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def read_time(path, format):\n",
    "    start = time.time()\n",
    "    _ = spark.read.format(format).load(path).count()\n",
    "    print(f\"{format.upper()} read time: {round(time.time() - start, 2)} sec\")\n",
    "\n",
    "read_time(\"output/day3_parquet\", \"parquet\")\n",
    "read_time(\"output/day3_orc\", \"orc\")\n",
    "read_time(\"output/day3_csv\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d27a24-5eae-470e-bb88-59510244c100",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_repart.write.mode(\"overwrite\").partitionBy(\"category\").parquet(\"output/day3_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da021188-40c1-4741-bc06-98c4e9c326a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
